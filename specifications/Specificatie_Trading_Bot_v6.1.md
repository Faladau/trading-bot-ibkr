# SpecificaÈ›ie funcÈ›ionalÄƒ â€” Trading Bot AI cu Interactive Brokers
## v6.1 â€” Agentul 1 (Data Collector) â€” ULTRA-GRANULAR

**Status**: READY FOR CURSOR IMPLEMENTATION  
**Data creÄƒrii**: 2026-01-17  
**Versiune**: 6.1  
**Faza**: SpecificaÈ›ie tehnicÄƒ detaliatÄƒ pentru **Agentul 1 (Data Collector)**

---

## STRUCTURA DOCUMENT

1. **Scop Agentul 1**
2. **Input / Output Contract**
3. **ResponsabilitÄƒÈ›i detaliate**
4. **SchemÄƒ de date (JSON, CSV)**
5. **PaÈ™i granulari de implementare**
6. **FuncÈ›ii publice cu semnÄƒturi**
7. **Pseudo-cod**
8. **Teste unitare**
9. **Cum testezi rezultatele**

---

## 1. SCOP AGENTUL 1 â€” DATA COLLECTOR

### 1.1 DefiniÈ›ie
Agentul 1 **colecteazÄƒ date brute de pe piaÈ›Äƒ** (preÈ›, volum, timestamp) din surse externe È™i le **normalizeazÄƒ Ã®ntr-un format unic**, fÄƒrÄƒ a lua nicio decizie de tranzacÈ›ionare.

### 1.2 Principii de design
- âœ… **IgnoranÈ›Äƒ de afacere**: Nu È™tie de strategie, risc, ordine.
- âœ… **Format unic**: Indiferent de sursÄƒ, outputul e identic.
- âœ… **Verifiable**: Oricine poate verifica datele Ã®n CSV.
- âœ… **Async-ready**: Merge cu AsyncIO pentru non-blocking.
- âœ… **Testabil independent**: Nu depinde de alÈ›i agenÈ›i.

### 1.3 Surse de date (prioritate)
| SursÄƒ | Tip | LatenÈ›Äƒ | Prioritate | Motiv |
|-------|-----|---------|-----------|-------|
| **IBKR (ib_insync)** | Live + Istoric | <100ms | â­â­â­â­â­ | Oficiale, RTH, test paper |
| **Yahoo Finance (yfinance)** | Istoric | 1-2s | â­â­â­â­ | Cross-check, backup |
| **Alpha Vantage** | Istoric | 2-5s | â­â­â­ | Backup, gratuit |
| **Stooq** | Istoric | 1s | â­â­â­ | AlternativÄƒ Yahoo |

---

## 2. INPUT / OUTPUT CONTRACT

### 2.1 INPUT (Ce primeÈ™te Agentul 1)

**Sursa**: FiÈ™ier configuraÈ›ie YAML

| Parameter | Tip | Exemplu | Mandatory | Descriere |
|-----------|-----|---------|-----------|-----------|
| `symbols` | List[str] | `["AAPL", "MSFT"]` | âœ… | Simbol-uri de tradat |
| `timeframe` | str | `"1H"`, `"1D"`, `"5m"` | âœ… | Granularitate date (IBKR format) |
| `lookback_days` | int | `60` | âœ… | Zile de date istorice de descÄƒrcat |
| `data_source` | str | `"IBKR"` | âœ… | SursÄƒ primarÄƒ |
| `backup_source` | str | `"YAHOO"` | âŒ | SursÄƒ secundarÄƒ la eÈ™ec |
| `output_format` | List[str] | `["csv", "json"]` | âœ… | Formate output |
| `data_dir` | str | `"data/processed"` | âœ… | Cale salvare output |
| `market` | str | `"US"` | âœ… | PiaÈ›Äƒ (US, EU, CRYPTO) |
| `useRTH` | bool | `True` | âœ… | Ore de trading regulate (IBKR) |
| `normalize_splits` | bool | `True` | âœ… | Ajustare split/dividend |

**Exemplu config.yaml:**
```yaml
data_collector:
  symbols:
    - AAPL
    - MSFT
    - AMD
  timeframe: "1H"
  lookback_days: 60
  data_source: "IBKR"
  backup_source: "YAHOO"
  output_format: ["csv", "json"]
  data_dir: "data/processed"
  market: "US"
  useRTH: true
  normalize_splits: true

ibkr:
  host: 127.0.0.1
  port: 7497
  clientId: 1
```

### 2.2 OUTPUT (Ce produce Agentul 1)

**DouÄƒ formate paralele:**

#### A. CSV (pentru verificare manualÄƒ)

**FiÈ™ier**: `data/processed/{SYMBOL}_{TIMEFRAME}_{DATE}.csv`

```csv
symbol,timeframe,timestamp,open,high,low,close,volume,count,wap,hasGaps,source,normalized
AAPL,1H,2026-01-17 10:00:00,150.50,151.00,150.20,150.80,1500000,450,150.65,False,IBKR,True
AAPL,1H,2026-01-17 11:00:00,150.80,151.50,150.60,151.20,1200000,420,151.00,False,IBKR,True
MSFT,1H,2026-01-17 10:00:00,380.50,381.00,380.20,380.80,800000,350,380.65,False,IBKR,True
```

**Coloane detaliate:**

| ColoanÄƒ | Tip | Descriere | Exemplu |
|---------|-----|-----------|---------|
| `symbol` | str | Simbol stoc | `AAPL` |
| `timeframe` | str | Timeframe | `1H`, `1D`, `5m` |
| `timestamp` | datetime | Ora UTC Ã®nchidere | `2026-01-17 10:00:00` |
| `open` | float | PreÈ› deschidere | `150.50` |
| `high` | float | PreÈ› maxim | `151.00` |
| `low` | float | PreÈ› minim | `150.20` |
| `close` | float | PreÈ› Ã®nchidere | `150.80` |
| `volume` | int | Volum total | `1500000` |
| `count` | int | Nr. tranzacÈ›ii Ã®n perioada | `450` |
| `wap` | float | PreÈ› mediu ponderat (volum) | `150.65` |
| `hasGaps` | bool | DacÄƒ bar are gap-uri | `False` |
| `source` | str | SursÄƒ date | `IBKR`, `YAHOO` |
| `normalized` | bool | DacÄƒ a fost normalizat | `True` |

#### B. JSON (pentru Agentul 2)

**FiÈ™ier**: `data/processed/{SYMBOL}_{TIMEFRAME}_{DATE}.json`

```json
{
  "symbol": "AAPL",
  "timeframe": "1H",
  "period": {
    "start": "2026-01-01 00:00:00",
    "end": "2026-01-17 23:59:59",
    "total_bars": 420,
    "date_generated": "2026-01-17 10:30:00"
  },
  "metadata": {
    "source": "IBKR",
    "useRTH": true,
    "normalized": true,
    "adjustments_applied": ["splits", "dividends"],
    "data_quality": {
      "missing_bars": 0,
      "gaps_detected": 0,
      "duplicates": 0
    }
  },
  "bars": [
    {
      "timestamp": "2026-01-17 10:00:00",
      "open": 150.50,
      "high": 151.00,
      "low": 150.20,
      "close": 150.80,
      "volume": 1500000,
      "count": 450,
      "wap": 150.65,
      "hasGaps": false
    },
    {
      "timestamp": "2026-01-17 11:00:00",
      "open": 150.80,
      "high": 151.50,
      "low": 150.60,
      "close": 151.20,
      "volume": 1200000,
      "count": 420,
      "wap": 151.00,
      "hasGaps": false
    }
  ]
}
```

### 2.3 Schema de validare

```python
# Output validation schema
OUTPUT_SCHEMA = {
    "symbol": str,           # Non-empty, uppercase
    "timeframe": str,        # "1m", "5m", "15m", "1H", "4H", "1D"
    "timestamp": datetime,   # UTC, valid market hours
    "open": float,           # > 0
    "high": float,           # >= high(open, close)
    "low": float,            # <= low(open, close)
    "close": float,          # > 0
    "volume": int,           # >= 0
    "count": int,            # > 0
    "wap": float,            # > 0
    "hasGaps": bool,         # True/False
    "source": str,           # "IBKR", "YAHOO", "ALPHA"
    "normalized": bool       # True/False
}
```

---

## 3. RESPONSABILITÄ‚ÈšI DETALIATE

### 3.1 IniÈ›ializare È™i conexiune
**PaÈ™i:**
1. Citire fiÈ™ier config YAML
2. Validare parametri config
3. IniÈ›ializare connector IBKR (ib_insync)
4. Conectare la TWS/IB Gateway (cu retry logic)
5. Verificare stare conexiune

**Pericol**: DacÄƒ nu reuÈ™eÈ™te conectarea, returneazÄƒ eroare clar È™i Ã®ncearcÄƒ backup source.

### 3.2 DescÄƒrcare date istorice
**PaÈ™i per simbol:**
1. CreeazÄƒ Contract object IBKR
2. ApeleazÄƒ `reqHistoricalData()` cu parametrii
3. AÈ™teaptÄƒ rÄƒspuns (poate dura secunde)
4. ValideazÄƒ completitudinea datelor
5. DetecteazÄƒ È™i logheazÄƒ gap-uri
6. NormalizeazÄƒ format

**Pacing limits IBKR** (IMPORTANT):
- Max 1 request / 10 secunde pentru rate-limit dur
- Soft limit pentru 1H, 1D: ~1-2 sec Ã®ntre requests OK
- Implement: sleep/exponential backoff Ã®ntre requests

### 3.3 Stream live (subscribe la bars)
**PaÈ™i:**
1. DupÄƒ descÄƒrcare istoric, subscribe la stream
2. `keepUpToDate=True` Ã®n ib_insync
3. PrimeÈ™te notificÄƒri pentru fiecare bar nou
4. Update Ã®n memorie (dict per simbol)
5. Periodic flush la CSV (ex: fiecare 5 minute)

### 3.4 Normalizare format
**PaÈ™i:**
1. Standardizare timestamp UTC
2. Conversie tipuri date (int, float)
3. Rotundire preÈ› la 2 zecimale
4. Validare OHLC logic (high >= max(open,close), etc.)
5. Detectare È™i marcaj anomalii

**Anomalii detectate:**
- Volume = 0 (invalid)
- High < Low (eroare date)
- WAP = 0 sau nenormal
- Price gaps > 5% (marca pauze piaÈ›Äƒ)

### 3.5 Salvare output
**PaÈ™i:**
1. Transformare Ã®n pandas DataFrame (optional)
2. Export CSV cu delimitatorul `,`
3. Export JSON pretty-print
4. Verificare fiÈ™iere create (file size > 0)
5. Logging detaliat

---

## 4. SCHEMÄ‚ DE DATE DETALIATÄ‚

### 4.1 Obiect Bar (ib_insync native)

Din `ib_insync`, un bar istoric are:
```python
from ib_insync import BarData

bar = {
    'time': datetime,     # Ora barei
    'open': float,
    'high': float,
    'low': float,
    'close': float,
    'volume': int,
    'average': float,     # WAP (weighted average price)
    'count': int,         # NumÄƒr tranzacÈ›ii
    'hasGaps': bool       # dacÄƒ sunt gaps Ã®n date
}
```

### 4.2 Transformare Ã®n standardul nostru

```python
# Raw ib_insync bar
raw_bar = ib.reqHistoricalData(...)[0]

# Normalizare
normalized_bar = {
    'symbol': 'AAPL',
    'timeframe': '1H',
    'timestamp': raw_bar.time.strftime('%Y-%m-%d %H:%M:%S'),  # UTC string
    'open': round(raw_bar.open, 2),
    'high': round(raw_bar.high, 2),
    'low': round(raw_bar.low, 2),
    'close': round(raw_bar.close, 2),
    'volume': int(raw_bar.volume),
    'count': int(raw_bar.count),
    'wap': round(raw_bar.average, 2),
    'hasGaps': bool(raw_bar.hasGaps),
    'source': 'IBKR',
    'normalized': True
}
```

### 4.3 Validare logicÄƒ OHLC

```
ValidÄƒri:
1. high >= max(open, close)       âœ… OK
2. low <= min(open, close)        âœ… OK
3. open > 0                        âœ… OK
4. high > low                      âœ… OK
5. close > 0                       âœ… OK
6. volume >= 0                     âœ… OK
7. timestamp validÄƒ UTC            âœ… OK
8. wap = (total_value / volume)    âœ… Calculat

DacÄƒ orice condiÈ›ie falsa â†’ Log ERROR + Mark bar cu `valid=False`
```

---

## 5. PAÈ˜I GRANULARI DE IMPLEMENTARE

### PASUL 1: StructurÄƒ foldere
```
agents/data_collector/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md
â”œâ”€â”€ collector.py                 # Entry point
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ibkr_source.py          # IBKR connector
â”‚   â”œâ”€â”€ yahoo_source.py         # Yahoo backup
â”‚   â””â”€â”€ base_source.py          # Abstract class
â”œâ”€â”€ normalizer.py               # Format standardizare
â”œâ”€â”€ validator.py                # Verificare date
â”œâ”€â”€ config.py                   # Citire config
â”œâ”€â”€ models.py                   # Type hints, dataclasses
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ibkr_source.py
â”‚   â”œâ”€â”€ test_normalizer.py
â”‚   â”œâ”€â”€ test_validator.py
â”‚   â””â”€â”€ test_output_format.py
â””â”€â”€ requirements.txt
```

### PASUL 2: Type hints È™i dataclasses

**FiÈ™ier**: `agents/data_collector/models.py`

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Optional

@dataclass
class Bar:
    """Standardizat bar data."""
    symbol: str
    timeframe: str
    timestamp: datetime
    open: float
    high: float
    low: float
    close: float
    volume: int
    count: int
    wap: float
    hasGaps: bool
    source: str
    normalized: bool

    def to_dict(self) -> Dict:
        return {
            'symbol': self.symbol,
            'timeframe': self.timeframe,
            'timestamp': self.timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'open': self.open,
            'high': self.high,
            'low': self.low,
            'close': self.close,
            'volume': self.volume,
            'count': self.count,
            'wap': self.wap,
            'hasGaps': self.hasGaps,
            'source': self.source,
            'normalized': self.normalized
        }

@dataclass
class DataCollectorConfig:
    """ConfiguraÈ›ie data collector."""
    symbols: List[str]
    timeframe: str
    lookback_days: int
    data_source: str
    backup_source: Optional[str]
    output_format: List[str]  # csv, json
    data_dir: str
    market: str
    useRTH: bool
    normalize_splits: bool
```

### PASUL 3: Clase abstract pentru sursele de date

**FiÈ™ier**: `agents/data_collector/sources/base_source.py`

```python
from abc import ABC, abstractmethod
from typing import List
from models import Bar

class BaseDataSource(ABC):
    """Abstract class pentru orice sursÄƒ de date."""
    
    @abstractmethod
    async def connect(self) -> bool:
        """ConecteazÄƒ la sursÄƒ."""
        pass
    
    @abstractmethod
    async def disconnect(self) -> bool:
        """DeconecteazÄƒ."""
        pass
    
    @abstractmethod
    async def fetch_historical_data(
        self, 
        symbol: str, 
        timeframe: str, 
        lookback_days: int
    ) -> List[Bar]:
        """DescarcÄƒ date istorice."""
        pass
    
    @abstractmethod
    async def subscribe_to_bars(
        self, 
        symbol: str, 
        timeframe: str
    ) -> None:
        """Subscribe la stream live (keepUpToDate)."""
        pass
    
    @abstractmethod
    def get_latest_bar(self, symbol: str) -> Optional[Bar]:
        """PrimeÈ™te ultimul bar din memorie."""
        pass
```

### PASUL 4: IBKR source

**FiÈ™ier**: `agents/data_collector/sources/ibkr_source.py`

```python
from ib_insync import IB, Stock, Contract
from typing import List, Optional
import logging
import asyncio
from datetime import datetime, timedelta
from models import Bar
from base_source import BaseDataSource

class IBKRDataSource(BaseDataSource):
    """Data source pentru Interactive Brokers."""
    
    def __init__(self, host: str, port: int, clientId: int):
        self.ib = IB()
        self.host = host
        self.port = port
        self.clientId = clientId
        self.bars_cache: Dict[str, Bar] = {}
        self.logger = logging.getLogger(__name__)
    
    async def connect(self) -> bool:
        """ConecteazÄƒ la IBKR cu retry logic."""
        retries = 3
        for attempt in range(retries):
            try:
                self.ib.connect(
                    self.host, 
                    self.port, 
                    clientId=self.clientId
                )
                self.logger.info(f"IBKR connected: {self.host}:{self.port}")
                return True
            except Exception as e:
                self.logger.warning(f"Connection attempt {attempt+1} failed: {e}")
                await asyncio.sleep(2 ** attempt)  # exponential backoff
        return False
    
    async def disconnect(self) -> bool:
        """DeconecteazÄƒ IBKR."""
        try:
            self.ib.disconnect()
            self.logger.info("IBKR disconnected")
            return True
        except Exception as e:
            self.logger.error(f"Disconnect error: {e}")
            return False
    
    async def fetch_historical_data(
        self, 
        symbol: str, 
        timeframe: str, 
        lookback_days: int,
        useRTH: bool = True
    ) -> List[Bar]:
        """
        DescarcÄƒ date istorice din IBKR.
        
        Args:
            symbol: ex 'AAPL'
            timeframe: ex '1H', '1D'
            lookback_days: zile de descÄƒrcat
            useRTH: ore regulate de trading
        
        Returns:
            Lista de Bar-uri normalizate
        """
        try:
            # CreeazÄƒ contract
            contract = Stock(symbol, 'SMART', 'USD')
            
            # Parametri
            duration_str = f"{lookback_days} D"
            bar_size = self._convert_timeframe(timeframe)  # ex: "1 hour"
            
            # Request
            self.logger.info(f"Fetching {symbol} {timeframe} for {lookback_days} days...")
            bars_raw = self.ib.reqHistoricalData(
                contract,
                endDateTime='',          # Until now
                durationStr=duration_str,
                barSizeSetting=bar_size,
                whatToShow='TRADES',     # Real trades
                useRTH=useRTH,
                keepUpToDate=False       # Historic only
            )
            
            # Convert È™i normalizare
            bars_normalized = []
            for bar in bars_raw:
                normalized = self._normalize_bar(bar, symbol, timeframe, 'IBKR')
                bars_normalized.append(normalized)
            
            self.logger.info(f"Fetched {len(bars_normalized)} bars for {symbol}")
            return bars_normalized
            
        except Exception as e:
            self.logger.error(f"Fetch error for {symbol}: {e}")
            return []
    
    async def subscribe_to_bars(
        self, 
        symbol: str, 
        timeframe: str
    ) -> None:
        """Subscribe la stream live."""
        try:
            contract = Stock(symbol, 'SMART', 'USD')
            bar_size = self._convert_timeframe(timeframe)
            
            # Request cu keepUpToDate=True
            bars = self.ib.reqHistoricalData(
                contract,
                endDateTime='',
                durationStr='1 D',       # Doar astazi + live
                barSizeSetting=bar_size,
                whatToShow='TRADES',
                useRTH=True,
                keepUpToDate=True        # LIVE STREAM
            )
            
            # Setup callback
            bars.updateEvent += lambda bar: self._on_bar_update(bar, symbol, timeframe)
            
            self.logger.info(f"Subscribed to {symbol} {timeframe} live bars")
        except Exception as e:
            self.logger.error(f"Subscribe error: {e}")
    
    def _on_bar_update(self, bar, symbol: str, timeframe: str):
        """Callback la update bar."""
        normalized = self._normalize_bar(bar, symbol, timeframe, 'IBKR')
        self.bars_cache[symbol] = normalized
        self.logger.debug(f"Bar update: {symbol} {normalized.close}")
    
    def get_latest_bar(self, symbol: str) -> Optional[Bar]:
        """Ultimul bar din cache."""
        return self.bars_cache.get(symbol)
    
    def _convert_timeframe(self, tf: str) -> str:
        """Conversia timeframe App â†’ IBKR."""
        mapping = {
            '1m': '1 min',
            '5m': '5 mins',
            '15m': '15 mins',
            '1H': '1 hour',
            '4H': '4 hours',
            '1D': '1 day'
        }
        return mapping.get(tf, '1 hour')
    
    def _normalize_bar(self, raw_bar, symbol: str, timeframe: str, source: str) -> Bar:
        """Normalizare raw bar â†’ Bar standardizat."""
        return Bar(
            symbol=symbol,
            timeframe=timeframe,
            timestamp=raw_bar.time,
            open=round(raw_bar.open, 2),
            high=round(raw_bar.high, 2),
            low=round(raw_bar.low, 2),
            close=round(raw_bar.close, 2),
            volume=int(raw_bar.volume),
            count=int(raw_bar.count),
            wap=round(raw_bar.average, 2),
            hasGaps=bool(raw_bar.hasGaps),
            source=source,
            normalized=True
        )
```

### PASUL 5: Normalizer

**FiÈ™ier**: `agents/data_collector/normalizer.py`

```python
from typing import List, Dict
import pandas as pd
from models import Bar
import logging

class DataNormalizer:
    """Normalizare format date."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def bars_to_csv(self, bars: List[Bar], filepath: str) -> bool:
        """ExportÄƒ bars â†’ CSV."""
        try:
            df = pd.DataFrame([bar.to_dict() for bar in bars])
            df.to_csv(filepath, index=False)
            self.logger.info(f"Saved {len(bars)} bars to {filepath}")
            return True
        except Exception as e:
            self.logger.error(f"CSV export error: {e}")
            return False
    
    def bars_to_json(self, bars: List[Bar], filepath: str, symbol: str, timeframe: str) -> bool:
        """ExportÄƒ bars â†’ JSON cu metadata."""
        try:
            import json
            from datetime import datetime
            
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'period': {
                    'start': bars[0].timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                    'end': bars[-1].timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_bars': len(bars),
                    'date_generated': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
                },
                'metadata': {
                    'source': bars[0].source if bars else 'UNKNOWN',
                    'normalized': True,
                    'data_quality': {
                        'missing_bars': self._count_missing_bars(bars),
                        'gaps_detected': sum(1 for b in bars if b.hasGaps),
                        'duplicates': self._count_duplicates(bars)
                    }
                },
                'bars': [bar.to_dict() for bar in bars]
            }
            
            with open(filepath, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            self.logger.info(f"Saved {len(bars)} bars to {filepath}")
            return True
        except Exception as e:
            self.logger.error(f"JSON export error: {e}")
            return False
    
    def _count_missing_bars(self, bars: List[Bar]) -> int:
        """DetecteazÄƒ baruri lipsÄƒ."""
        # CalculeazÄƒ expected count vs actual
        # Implementare simpla pentru start
        return 0
    
    def _count_duplicates(self, bars: List[Bar]) -> int:
        """DetecteazÄƒ duplicate timestamp."""
        timestamps = [b.timestamp for b in bars]
        return len(timestamps) - len(set(timestamps))
```

### PASUL 6: Validator

**FiÈ™ier**: `agents/data_collector/validator.py`

```python
from typing import List, Tuple
from models import Bar
import logging

class DataValidator:
    """Validare calitate date."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def validate_bar(self, bar: Bar) -> Tuple[bool, str]:
        """ValideazÄƒ un bar individual."""
        errors = []
        
        # OHLC logic
        if bar.high < bar.open or bar.high < bar.close:
            errors.append(f"High ({bar.high}) < open/close")
        
        if bar.low > bar.open or bar.low > bar.close:
            errors.append(f"Low ({bar.low}) > open/close")
        
        if bar.high < bar.low:
            errors.append(f"High ({bar.high}) < Low ({bar.low})")
        
        # Price valid
        if bar.open <= 0 or bar.close <= 0:
            errors.append("Price <= 0")
        
        # Volume
        if bar.volume < 0:
            errors.append("Volume < 0")
        
        # WAP logic
        if bar.wap <= 0 and bar.volume > 0:
            errors.append("WAP invalid")
        
        if errors:
            self.logger.warning(f"Bar validation errors: {errors}")
            return False, "; ".join(errors)
        
        return True, "OK"
    
    def validate_bars(self, bars: List[Bar]) -> Tuple[int, int]:
        """ValideazÄƒ lista de bars.
        
        Returns:
            (valid_count, invalid_count)
        """
        valid, invalid = 0, 0
        for bar in bars:
            is_valid, msg = self.validate_bar(bar)
            if is_valid:
                valid += 1
            else:
                invalid += 1
        
        return valid, invalid
```

### PASUL 7: Collector (Main orchestrator)

**FiÈ™ier**: `agents/data_collector/collector.py`

```python
from typing import List, Optional
import asyncio
import yaml
import logging
from pathlib import Path
from models import Bar, DataCollectorConfig
from sources.ibkr_source import IBKRDataSource
from normalizer import DataNormalizer
from validator import DataValidator
from config import load_config

class DataCollector:
    """Orchestrator principal pentru colectare date."""
    
    def __init__(self, config_path: str):
        self.config = load_config(config_path)
        self.data_source: Optional[IBKRDataSource] = None
        self.normalizer = DataNormalizer()
        self.validator = DataValidator()
        self.logger = logging.getLogger(__name__)
    
    async def initialize(self) -> bool:
        """IniÈ›ializare conexiuni È™i setup."""
        try:
            # ConecteazÄƒ la IBKR
            self.data_source = IBKRDataSource(
                host=self.config.get('ibkr.host'),
                port=self.config.get('ibkr.port'),
                clientId=self.config.get('ibkr.clientId')
            )
            
            connected = await self.data_source.connect()
            if not connected:
                self.logger.error("Failed to connect to IBKR")
                return False
            
            # CreazÄƒ output directories
            Path(self.config['data_dir']).mkdir(parents=True, exist_ok=True)
            
            self.logger.info("DataCollector initialized successfully")
            return True
        except Exception as e:
            self.logger.error(f"Initialization error: {e}")
            return False
    
    async def collect_all(self) -> bool:
        """ColecteazÄƒ date pentru toate simbolurile."""
        try:
            symbols = self.config['symbols']
            timeframe = self.config['timeframe']
            lookback_days = self.config['lookback_days']
            
            for symbol in symbols:
                self.logger.info(f"Collecting {symbol}...")
                
                # Fetch
                bars = await self.data_source.fetch_historical_data(
                    symbol=symbol,
                    timeframe=timeframe,
                    lookback_days=lookback_days,
                    useRTH=self.config['useRTH']
                )
                
                if not bars:
                    self.logger.warning(f"No data for {symbol}")
                    continue
                
                # Validate
                valid_count, invalid_count = self.validator.validate_bars(bars)
                self.logger.info(f"{symbol}: {valid_count} valid, {invalid_count} invalid")
                
                # Save
                await self._save_bars(symbol, bars)
                
                # Pace limit IBKR (min 10 sec Ã®ntre requests)
                await asyncio.sleep(10)
            
            self.logger.info("Collection completed")
            return True
        except Exception as e:
            self.logger.error(f"Collection error: {e}")
            return False
    
    async def _save_bars(self, symbol: str, bars: List[Bar]) -> bool:
        """SalveazÄƒ bars Ã®n format CSV + JSON."""
        try:
            timeframe = self.config['timeframe']
            data_dir = self.config['data_dir']
            
            # Filename
            from datetime import datetime
            date_str = datetime.utcnow().strftime('%Y%m%d')
            base_name = f"{data_dir}/{symbol}_{timeframe}_{date_str}"
            
            # CSV
            if 'csv' in self.config['output_format']:
                csv_path = f"{base_name}.csv"
                self.normalizer.bars_to_csv(bars, csv_path)
            
            # JSON
            if 'json' in self.config['output_format']:
                json_path = f"{base_name}.json"
                self.normalizer.bars_to_json(bars, json_path, symbol, timeframe)
            
            return True
        except Exception as e:
            self.logger.error(f"Save error: {e}")
            return False
    
    async def shutdown(self) -> bool:
        """Dezactivare controlatÄƒ."""
        try:
            if self.data_source:
                await self.data_source.disconnect()
            self.logger.info("DataCollector shutdown completed")
            return True
        except Exception as e:
            self.logger.error(f"Shutdown error: {e}")
            return False

# Entry point
async def main(config_path: str):
    collector = DataCollector(config_path)
    if await collector.initialize():
        await collector.collect_all()
        await collector.shutdown()
    else:
        print("Failed to initialize collector")

if __name__ == "__main__":
    import sys
    config_file = sys.argv[1] if len(sys.argv) > 1 else "config/config.yaml"
    asyncio.run(main(config_file))
```

---

## 6. FUNCÈšII PUBLICE â€” SEMNÄ‚TURI

### 6.1 IBKRDataSource API

```python
# Connect
async def connect() -> bool

# Disconnect
async def disconnect() -> bool

# Fetch historic data
async def fetch_historical_data(
    symbol: str, 
    timeframe: str, 
    lookback_days: int,
    useRTH: bool = True
) -> List[Bar]

# Subscribe live
async def subscribe_to_bars(
    symbol: str, 
    timeframe: str
) -> None

# Get latest bar from cache
def get_latest_bar(symbol: str) -> Optional[Bar]
```

### 6.2 DataNormalizer API

```python
# Export to CSV
def bars_to_csv(bars: List[Bar], filepath: str) -> bool

# Export to JSON
def bars_to_json(
    bars: List[Bar], 
    filepath: str, 
    symbol: str, 
    timeframe: str
) -> bool
```

### 6.3 DataValidator API

```python
# Validate single bar
def validate_bar(bar: Bar) -> Tuple[bool, str]

# Validate list of bars
def validate_bars(bars: List[Bar]) -> Tuple[int, int]  # (valid, invalid)
```

### 6.4 DataCollector API

```python
# Initialize
async def initialize() -> bool

# Collect all symbols
async def collect_all() -> bool

# Save bars
async def _save_bars(symbol: str, bars: List[Bar]) -> bool

# Shutdown
async def shutdown() -> bool
```

---

## 7. PSEUDO-COD â€” FLUXUL PRINCIPAL

```
FUNCTION main():
    
    // 1. SETUP
    config = load_config("config/config.yaml")
    collector = DataCollector(config)
    
    // 2. INITIALIZE
    IF NOT await collector.initialize():
        EXIT WITH ERROR "Connection failed"
    
    // 3. COLLECT PER SYMBOL
    FOR EACH symbol IN config.symbols:
        
        // 3a. Fetch
        bars_raw = await data_source.fetch_historical_data(
            symbol,
            config.timeframe,
            config.lookback_days
        )
        
        // 3b. Validate
        (valid_count, invalid_count) = validator.validate_bars(bars_raw)
        LOG "Symbol={symbol}, Valid={valid_count}, Invalid={invalid_count}"
        
        // 3c. Save
        IF 'csv' IN config.output_format:
            normalizer.bars_to_csv(bars_raw, f"data/{symbol}.csv")
        IF 'json' IN config.output_format:
            normalizer.bars_to_json(bars_raw, f"data/{symbol}.json", symbol, config.timeframe)
        
        // 3d. Pace limit (IBKR min 10 sec)
        SLEEP 10 seconds
    
    // 4. SUBSCRIBE LIVE (opÈ›ional, pentru monitoring)
    FOR EACH symbol IN config.symbols:
        SUBSCRIBE TO data_source.subscribe_to_bars(symbol, config.timeframe)
    
    // 5. SHUTDOWN
    await collector.shutdown()
    
    RETURN SUCCESS
```

---

## 8. TESTE UNITARE

**FiÈ™ier**: `agents/data_collector/tests/test_ibkr_source.py`

```python
import pytest
import asyncio
from models import Bar
from sources.ibkr_source import IBKRDataSource
from datetime import datetime

@pytest.mark.asyncio
async def test_connect():
    """Test IBKR connection."""
    source = IBKRDataSource("127.0.0.1", 7497, 1)
    result = await source.connect()
    assert result == True, "Connection failed"
    await source.disconnect()

@pytest.mark.asyncio
async def test_fetch_historical_data():
    """Test descÄƒrcare date istorice."""
    source = IBKRDataSource("127.0.0.1", 7497, 1)
    await source.connect()
    
    bars = await source.fetch_historical_data(
        symbol="AAPL",
        timeframe="1H",
        lookback_days=5,
        useRTH=True
    )
    
    assert len(bars) > 0, "No bars fetched"
    assert all(isinstance(b, Bar) for b in bars), "Invalid bar type"
    
    await source.disconnect()

@pytest.mark.asyncio
async def test_bar_normalization():
    """Test normalizare bar."""
    source = IBKRDataSource("127.0.0.1", 7497, 1)
    await source.connect()
    
    bars = await source.fetch_historical_data("AMD", "1D", 10)
    for bar in bars:
        assert bar.symbol == "AMD"
        assert bar.timeframe == "1D"
        assert bar.normalized == True
    
    await source.disconnect()
```

**FiÈ™ier**: `agents/data_collector/tests/test_normalizer.py`

```python
import pytest
from normalizer import DataNormalizer
from models import Bar
from datetime import datetime
import os

def test_bars_to_csv():
    """Test export CSV."""
    normalizer = DataNormalizer()
    
    bars = [
        Bar(
            symbol="AAPL",
            timeframe="1H",
            timestamp=datetime(2026, 1, 17, 10, 0, 0),
            open=150.50,
            high=151.00,
            low=150.20,
            close=150.80,
            volume=1500000,
            count=450,
            wap=150.65,
            hasGaps=False,
            source="IBKR",
            normalized=True
        )
    ]
    
    result = normalizer.bars_to_csv(bars, "/tmp/test.csv")
    assert result == True
    assert os.path.exists("/tmp/test.csv")
    
    os.remove("/tmp/test.csv")

def test_bars_to_json():
    """Test export JSON."""
    normalizer = DataNormalizer()
    
    bars = [
        Bar(
            symbol="AAPL",
            timeframe="1H",
            timestamp=datetime(2026, 1, 17, 10, 0, 0),
            open=150.50,
            high=151.00,
            low=150.20,
            close=150.80,
            volume=1500000,
            count=450,
            wap=150.65,
            hasGaps=False,
            source="IBKR",
            normalized=True
        )
    ]
    
    result = normalizer.bars_to_json(bars, "/tmp/test.json", "AAPL", "1H")
    assert result == True
    assert os.path.exists("/tmp/test.json")
    
    os.remove("/tmp/test.json")
```

**FiÈ™ier**: `agents/data_collector/tests/test_validator.py`

```python
from validator import DataValidator
from models import Bar
from datetime import datetime

def test_validate_valid_bar():
    """Test validare bar OK."""
    validator = DataValidator()
    
    bar = Bar(
        symbol="AAPL",
        timeframe="1H",
        timestamp=datetime.now(),
        open=150.00,
        high=151.00,
        low=149.00,
        close=150.50,
        volume=1000000,
        count=400,
        wap=150.20,
        hasGaps=False,
        source="IBKR",
        normalized=True
    )
    
    is_valid, msg = validator.validate_bar(bar)
    assert is_valid == True

def test_validate_invalid_bar_high_low():
    """Test validare bar invalid (high < low)."""
    validator = DataValidator()
    
    bar = Bar(
        symbol="AAPL",
        timeframe="1H",
        timestamp=datetime.now(),
        open=150.00,
        high=149.00,  # INVALID
        low=151.00,
        close=150.50,
        volume=1000000,
        count=400,
        wap=150.20,
        hasGaps=False,
        source="IBKR",
        normalized=True
    )
    
    is_valid, msg = validator.validate_bar(bar)
    assert is_valid == False
```

---

## 9. CUM TESTEZI REZULTATELE

### 9.1 Test local (development)

```bash
# 1. Setup
cd agents/data_collector
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Configurare (editeazÄƒ config.yaml)
nano ../../config/config.yaml

# 3. Rulare data collector
python collector.py ../../config/config.yaml

# 4. Verificare CSV
cat data/AAPL_1H_20260117.csv

# 5. Verificare JSON
cat data/AAPL_1H_20260117.json

# 6. Run tests
pytest tests/ -v

# 7. Check data quality
python -c "
import pandas as pd
df = pd.read_csv('data/AAPL_1H_20260117.csv')
print(f'Rows: {len(df)}')
print(f'Columns: {df.columns.tolist()}')
print(f'Missing values: {df.isnull().sum().sum()}')
print(df.head())
"
```

### 9.2 Checklist de validare manualÄƒ

```
[ ] Datele sunt descÄƒrcate (CSV È™i JSON create)
[ ] CSV are coloane corecte: symbol, timestamp, open, high, low, close, volume, etc.
[ ] Timestamp-uri sunt Ã®n ordine crescÄƒtoare (nici o inversie)
[ ] High >= max(open, close) pentru fiecare bar
[ ] Low <= min(open, close) pentru fiecare bar
[ ] Volume > 0 pentru fiecare bar
[ ] WAP > 0 pentru fiecare bar
[ ] Nici o valoare NaN sau Inf
[ ] CSV se deschide Ã®n Excel fÄƒrÄƒ erori
[ ] JSON este valid JSON (nu ai erori de parsing)
[ ] Timestamp-uri sunt Ã®n UTC
[ ] Nu au gap-uri neaÈ™teptate Ã®n timpuri (ex: 10:00 â†’ 12:00 la 1H)
[ ] Dimensiunea fiÈ™ierelor e rezonabilÄƒ (nu 0 bytes)
```

### 9.3 Test pe mai mulÈ›i agenÈ›i

DupÄƒ ce Agentul 1 e validat:

```bash
# 1. GenereazÄƒ CSV/JSON de test
python collector.py config/config.yaml

# 2. CopiazÄƒ CSV Ã®n input Agentului 2
cp data/AAPL_1H_*.csv ../strategy_engine/data/

# 3. Agentul 2 citeÈ™te din CSV È™i produce semnale
python ../strategy_engine/strategy.py

# 4. VerificÄƒ semnale
cat ../strategy_engine/output/AAPL_signals.json
```

---

## 10. RESURSE È˜I REFERINÈšE

### Documentare
- [ib_insync API Docs](https://ib-insync.readthedocs.io/api.html) [web:67]
- [Interactive Brokers Python API](https://www.interactivebrokers.com/campus/trading-lessons/python-receiving-market-data/) [web:68]
- [ib_insync Historical Data](https://algotrading101.com/learn/ib_insync-interactive-brokers-api-guide/) [web:71]
- [OHLCV Data Best Practices](https://www.coinapi.io/blog/ohlcv-data-explained-real-time-updates-websocket-behavior-and-trading-applications) [web:73]

### Pacing Limits
- [IBKR Pacing Limits](https://interactivebrokers.github.io/tws-api/historical_limitations.html) â€” Max 1 req / 10 sec [web:77]
- Soft limit pentru 1H+: ~1-2 sec OK

### IntegrÄƒri alternative
- Yahoo Finance: `yfinance` library (backup)
- Alpha Vantage: API gratuit (backup)
- Stooq: Source alternativÄƒ (backup)

---

## 11. UI DASHBOARD â€” STREAMLIT

### 11.1 Scop Dashboard
Dashboard responsive (mobile + desktop) pentru **monitorizare** trading bot. Optimizat pentru verificare 1-2x pe zi, nu pentru trading live continuu.

**FuncÈ›ionalitÄƒÈ›i principale:**
- âœ… **Background atractiv** â€” gradient modern, dark theme, nu alb plictisitor
- âœ… Status agenÈ›i (Agent 1, 2, 3) â€” ACTIVE/IDLE/MONITORING/ERROR
- âœ… Live market data (citeÈ™te din CSV-uri generate de Agent 1)
- âœ… **Metrici esenÈ›iale** â€” P&L Total, Win Rate, Max Drawdown, PoziÈ›ii Active
- âœ… Activity logs (ultimele acÈ›iuni)
- âœ… Configuration display (read-only)
- âœ… Responsive design (funcÈ›ioneazÄƒ pe telefon)
- âœ… Deploy GRATUIT pe Streamlit Cloud

**Focus:**
- **Monitorizare** â€” vezi status, performanÈ›Äƒ, poziÈ›ii
- **Verificare 1-2x pe zi** â€” nu necesitÄƒ refresh continuu
- **Vizual atractiv** â€” background gradient, culori moderne, design clean

### 11.2 StructurÄƒ Dashboard

**Entry Point**: `streamlit_app.py` (root)
**Main Module**: `src/ui/dashboard.py`

**Layout (2x2 Grid):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trading Bot v6.2 Dashboard            â”‚
â”‚  [Agent 1] [Agent 2] [Agent 3]        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Live Market Data â”‚ Performance Metrics  â”‚
â”‚ (Top Left)       â”‚ (Top Right)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Controls         â”‚ Activity Logs        â”‚
â”‚ (Bottom Left)    â”‚ (Bottom Right)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.3 Componente Dashboard

#### 11.3.1 Status AgenÈ›i
**LocaÈ›ie**: Top row (3 coloane)

**Status-uri posibile:**
- ğŸŸ¢ **ACTIVE** â€” Agent ruleazÄƒ (verde)
- ğŸŸ¡ **IDLE** â€” Agent inactiv (galben)
- ğŸ”µ **MONITORING** â€” Agent monitorizeazÄƒ (albastru)
- ğŸ”´ **ERROR** â€” Eroare (roÈ™u)

**AgenÈ›i:**
- **Agent 1**: Data Collection
- **Agent 2**: Decision
- **Agent 3**: Execution

#### 11.3.2 Live Market Data
**LocaÈ›ie**: Top Left

**FuncÈ›ionalitate:**
- CiteÈ™te ultimele date din `data/processed/*.csv`
- AfiÈ™eazÄƒ pentru fiecare simbol:
  - PreÈ› curent (close)
  - Change % (delta)
  - Mini chart placeholder
- DacÄƒ nu sunt date: mesaj "Nu sunt date disponibile. RuleazÄƒ Agent 1 pentru a colecta date."

**SursÄƒ date**: CSV-uri generate de Agent 1

#### 11.3.3 Performance Metrics
**LocaÈ›ie**: Top Right

**Metrici esenÈ›iale (prioritizate):**
- **Total PnL** â€” Profit/Pierdere totalÄƒ (CEA MAI IMPORTANTÄ‚)
- **Win Rate** â€” Procent trade-uri profitabile
- **Max Drawdown** â€” Cea mai mare scÄƒdere (indicator de risc)
- **PoziÈ›ii Active** â€” NumÄƒr poziÈ›ii deschise acum

**Metrici secundare (opÈ›ional):**
- Daily PnL, Weekly PnL, Sharpe Ratio

**SursÄƒ date**: `data/trades/*.json` (trade-uri completate)

**Design:**
- Metrici principale Ã®n carduri mari, vizibile
- Color coding: verde (profit), roÈ™u (pierdere)
- Iconuri pentru claritate vizualÄƒ

#### 11.3.4 Controls (OpÈ›ional - Focus pe Monitorizare)
**LocaÈ›ie**: Bottom Left (secundar)

**NotÄƒ:** Dashboard-ul este optimizat pentru **monitorizare**, nu pentru control activ. Butoanele sunt disponibile dar nu sunt prioritare.

**Butoane:**
- **â–¶ï¸ START** (Primary) â€” PorneÈ™te Agent 1 Ã®n background
  - CreeazÄƒ thread separat
  - RuleazÄƒ `DataCollectionAgent.collect_all()`
  - ColecteazÄƒ date de la Yahoo Finance (sau IBKR dacÄƒ e configurat)
  - SalveazÄƒ CSV-uri Ã®n `data/processed/`
  - ActualizeazÄƒ status la ACTIVE â†’ IDLE
  
- **â¹ï¸ STOP** (Secondary) â€” OpreÈ™te bot-ul
  - SeteazÄƒ toÈ›i agenÈ›ii la IDLE
  - OpreÈ™te execuÈ›ia

**Configuration Display (Read-Only):**
- Mode: PAPER/LIVE
- Risk Level: Medium
- Max Position: $50k
- Stop Loss: 2%

#### 11.3.5 Activity Logs
**LocaÈ›ie**: Bottom Right

**FuncÈ›ionalitate:**
- AfiÈ™eazÄƒ ultimele 10 activitÄƒÈ›i
- Format: `**HH:MM:SS**: Agent X: Message`
- SursÄƒ: `data/signals/*.json` + logs simulate
- Auto-update cÃ¢nd bot-ul ruleazÄƒ

**Exemplu logs:**
```
14:32:15: Agent 1: BUY AAPL x100 @ $175.32
14:30:42: Agent 3: Market scan completed
14:28:09: Agent 2: Position closed +$234
14:25:33: Agent 1: SELL MSFT x50 @ $412.85
14:22:18: System: All agents initialized
```

### 11.4 Design Vizual

#### 11.4.1 Background Atractiv
**Gradient Modern:**
- Background: gradient linear de la `#0f0c29` (dark blue) â†’ `#302b63` (purple) â†’ `#24243e` (dark)
- Sau: gradient de la `#1e3c72` (blue) â†’ `#2a5298` (lighter blue)
- Carduri: background semi-transparent cu blur (`rgba(255, 255, 255, 0.1)` cu `backdrop-filter: blur(10px)`)
- Border: subtle glow pentru carduri importante

**Dark Theme:**
- Text: alb/light gray pentru contrast
- Accent colors: verde (profit), roÈ™u (pierdere), albastru (info)
- Shadows: subtle pentru depth

#### 11.4.2 Layout Optimizat pentru Verificare 1-2x pe Zi
- **Header**: Status agenÈ›i + ultima actualizare (mare, vizibil)
- **Metrici principale**: Carduri mari cu Total PnL, Win Rate, Max Drawdown
- **PoziÈ›ii active**: ListÄƒ clarÄƒ cu P&L per poziÈ›ie
- **Activity log**: Ultimele 10-15 acÈ›iuni (nu necesitÄƒ scroll infinit)

**Nu necesitÄƒ:**
- Auto-refresh agresiv (doar manual sau opÈ›ional)
- Real-time updates (datele se actualizeazÄƒ cÃ¢nd ruleazÄƒ Agent 1)

#### 11.4.3 FuncÈ›ionalitÄƒÈ›i Tehnice

**ExecuÈ›ie Agent 1:**
```python
# CÃ¢nd se apasÄƒ START (opÈ›ional)
thread = threading.Thread(target=run_agent1, daemon=True)
thread.start()
```

**Refresh:**
- **Manual**: Buton "ğŸ”„ Refresh" cÃ¢nd vrei sÄƒ verifici
- **OpÈ›ional**: Checkbox "Auto-refresh (60s)" pentru verificare periodicÄƒ
- **Nu**: Refresh continuu (nu e necesar pentru verificare 1-2x pe zi)

**Path Handling:**
- `config/config.yaml` (local development)
- `config.yaml` (root)
- Absolute path (Streamlit Cloud)
- Default config dacÄƒ nu gÄƒseÈ™te

**Responsive Design:**
- Mobile: Stack vertical, carduri full-width
- Desktop: 2-3 coloane, layout optimizat
- CSS: Gradient background, carduri glassmorphism

### 11.5 Deploy Streamlit Cloud

#### 11.5.1 Setup
1. **Repository**: `Faladau/trading-bot-ibkr`
2. **Branch**: `master`
3. **Main file**: `streamlit_app.py`
4. **URL**: `trading-bot-ibkr-*.streamlit.app`

#### 11.5.2 FiÈ™iere Necesare
- âœ… `streamlit_app.py` â€” Entry point
- âœ… `requirements.txt` â€” DependenÈ›e (streamlit>=1.28.1)
- âœ… `.streamlit/config.toml` â€” ConfiguraÈ›ie tema
- âœ… `packages.txt` â€” Gol (sau pachete system)
- âœ… `src/ui/dashboard.py` â€” Dashboard principal

#### 11.5.3 LimitÄƒri Streamlit Cloud
âš ï¸ **Important:**
- **FiÈ™ierele nu persistÄƒ** â€” CSV-urile se È™terg la restart
- **SoluÈ›ii:**
  1. FoloseÈ™te Streamlit Secrets pentru path-uri cloud (S3, Google Drive)
  2. Sau ruleazÄƒ Agent 1 local È™i upload CSV-uri manual
  3. Sau foloseÈ™te database cloud (PostgreSQL, MongoDB)

#### 11.5.4 Config Ã®n Streamlit Cloud
**OpÈ›ional â€” Streamlit Secrets:**
```toml
# .streamlit/secrets.toml (Ã®n Streamlit Cloud Settings)
[paths]
config_file = "config/config.yaml"
data_dir = "data/processed"

[ibkr]
host = "127.0.0.1"
port = 7497
```

### 11.6 Testare Dashboard

#### 11.6.1 Local
```bash
streamlit run streamlit_app.py
```
Dashboard se deschide: `http://localhost:8501`

#### 11.6.2 Streamlit Cloud
1. Push pe GitHub
2. Streamlit Cloud detecteazÄƒ automat
3. AcceseazÄƒ URL generat
4. TesteazÄƒ butonul START

### 11.7 Status Implementare

âœ… **Implementat:**
- Dashboard UI complet
- Status agenÈ›i
- Live market data (citire CSV)
- Performance metrics
- Controls (START/STOP funcÈ›ional)
- Activity logs
- Responsive design
- Auto-refresh
- Path handling robust

âš ï¸ **LimitÄƒri:**
- CSV-urile nu persistÄƒ Ã®n Streamlit Cloud (se È™terg)
- Agent 1 ruleazÄƒ Ã®n thread (nu persistÄƒ dupÄƒ refresh)
- Metrics sunt simulate (necesitÄƒ trade-uri reale)

### 11.8 UrmÄƒtorii PaÈ™i

1. **PersistenÈ›Äƒ date**: Integrare cu cloud storage (S3, Google Drive)
2. **Real-time updates**: WebSocket pentru updates live
3. **Charts**: Grafice interactive pentru preÈ›uri
4. **Alerts**: NotificÄƒri push (Telegram, email)
5. **Backtesting UI**: InterfaÈ›Äƒ pentru backtesting

---

## CONCLUZIE

## CONCLUZIE

**Agentul 1 este ultra-granular È™i ready for Cursor implementation.**

- âœ… ResponsabilitÄƒÈ›i clare
- âœ… Input/output perfect definite
- âœ… FuncÈ›ii publice cu semnÄƒturi
- âœ… Pseudo-cod detaliat
- âœ… Teste incluse
- âœ… Validare manually verificabilÄƒ

**Next step**: Implementare Ã®n Cursor, apoi Agentul 2.

---

**Document**: SpecificaÈ›ie Agentul 1 â€” Data Collector v6.1  
**Status**: PRODUCTION READY  
**Ultima actualizare**: 2026-01-17  
**Autor**: AI Assistant + Developer Feedback